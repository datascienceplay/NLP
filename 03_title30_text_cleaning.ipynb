{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dr = pd.read_pickle('data/covid19_reference_count.pickle')\n",
    "title = pd.read_pickle('data/covid19_title.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Series.keys of 1        [Population-Based Study of the Influence of th...\n",
       "13       [Immunophenotyping of COVID-19 and influenza h...\n",
       "18       [Incidental COVID-19 related lung apical findi...\n",
       "32       [Studies of Novel Coronavirus Disease 19 (COVI...\n",
       "42       [Infodemiological study on COVID-19 epidemic a...\n",
       "58       [Clinical Features of COVID-19 in Patients Wit...\n",
       "67       [Coping with COVID-19: Exposure to COVID-19 an...\n",
       "95       [Characteristics associated with hospitalisati...\n",
       "104      [The Need for the Right Socio-Economic and Cul...\n",
       "109      [COVID-19, Australia: Epidemiology Report 7: R...\n",
       "114      [Detection dogs as a help in the detection of ...\n",
       "117      [COVID-19, Australia: Epidemiology Report 3: R...\n",
       "125      [COVID-19, Australia: Epidemiology Report 19 (...\n",
       "141                  [A scientometric overview of CORD-19]\n",
       "150      [COVID-19: to be or not to be; that is the dia...\n",
       "170                       [Emerging Neurology of COVID-19]\n",
       "171          [COVID-19 and Rheumatology: so far, so close]\n",
       "180      [Anesthesia and COVID-19: What We Should Know ...\n",
       "201      [The impact of the COVID-19 pandemic on suicid...\n",
       "209      [Cognitive, Affective, and Behavioral Construc...\n",
       "211      [Questionnaire on Perception of Threat from CO...\n",
       "215      [Prognostic Value of Cardiovascular Biomarkers...\n",
       "220      [Grocery Shopping Preferences during the COVID...\n",
       "226      [The Impact of COVID-19 on HIV Treatment and R...\n",
       "233      [Ophthalmology practice during the COVID-19 pa...\n",
       "238      [Epidemiology of mental health problems in COV...\n",
       "269         [COVID-19 Pandemic and Cardiovascular Disease]\n",
       "277      [Effects of COVID-19 on Indian Energy Consumpt...\n",
       "288        [The COVID-19 pandemic and health inequalities]\n",
       "306      [Preprinting a pandemic: the role of preprints...\n",
       "                               ...                        \n",
       "46994    [Literacy in the new norm: stay-home game plan...\n",
       "46999    [SARS-CoV-2 transcriptome analysis and molecul...\n",
       "47000    [Recognizing and realizing the value of custom...\n",
       "47003    [Significant expression of FURIN and ACE2 on o...\n",
       "47004    [Guidelines and tools for promoting digital eq...\n",
       "47005    [Genomic modeling as an approach to identify s...\n",
       "47015    [Early temporal dynamics of cellular responses...\n",
       "47017    [In silico Proteome analysis of Severe acute r...\n",
       "47018    [BioLaboro: A bioinformatics system for detect...\n",
       "47020    [Credibility of self-reported health parameter...\n",
       "47022    [Enablers for the adoption of e-health solutio...\n",
       "47024    [Analysis of SARS-CoV-2 RNA-Sequences by Inter...\n",
       "47027                 [The UCSC SARS-CoV-2 Genome Browser]\n",
       "47029    [Sequence-based prediction of vaccine targets ...\n",
       "47031    [Promoting Health and Well-Being through Mobil...\n",
       "47032    [Peer to peer health in older adults’ online c...\n",
       "47035    [Revisión sistemática del embarazo y la infecc...\n",
       "47036    [Ultra-Low-Cost Integrated Silicon-based Trans...\n",
       "47038    [Tablet-based Telerehabilitation Versus Conven...\n",
       "47047    [The Effects of Online Social Connectedness on...\n",
       "47051    [Improvements in Patient Monitoring for the In...\n",
       "47052    [Raising the Digital Profile of Facial Palsy: ...\n",
       "47054    [Structure and performance of the Italian alpi...\n",
       "47055    [SARS-CoV-2 Evolutionary Adaptation toward Hos...\n",
       "47056    [Effectiveness of an Eight-Week Web-Based Mind...\n",
       "47057             [Climatic-niche evolution of SARS CoV-2]\n",
       "47059    [Key Factors Influencing Purchase or Rent Deci...\n",
       "47063    [“We aren't your reincarnation!” workplace mot...\n",
       "47068    [The SARS-CoV-2 receptor, Angiotensin converti...\n",
       "47073    [İnsan-COVID-19'da Pandemik SARS Coronavirus-2...\n",
       "Name: title, Length: 4720, dtype: object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pick up reference count more than 30 times\n",
    "title30 = title[dr>=30]\n",
    "title30.keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text pre-processing - data cleaning part 1\n",
    "\n",
    "* Make text all lower case\n",
    "* Remove punctuation\n",
    "* Remove numerical values\n",
    "* Remove common non-sensical text (/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text = title30.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a first round of text cleaning techniques\n",
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text_round1(text):\n",
    "    '''Make text lowercase, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "round1 = lambda x: clean_text_round1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the updated text\n",
    "clean_data1 = data_text.apply(round1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a second round of cleaning\n",
    "def clean_text_round2(text):\n",
    "    '''Get rid of some additional punctuation and non-sensical text that was missed the first time around.'''\n",
    "    text = re.sub('[‘’“”…]', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    return text\n",
    "\n",
    "round2 = lambda x: clean_text_round2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the updated text\n",
    "clean_data2 = clean_data1.apply(round2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organizing the data\n",
    "\n",
    "We will organise data into two standard text format:\n",
    "\n",
    "1. Corpus - a collection of text\n",
    "2. Document-Term Matrix - words counts in matrix format\n",
    "\n",
    "### Corpus\n",
    "\n",
    "We already created a corpus in an earlier step. The definition of a corpus is a collection of texts, and they are all put together neatly in a pandas series here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pickle it for later use\n",
    "data_text.to_pickle(\"data/title30_corpus.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document-Term Matrix\n",
    "\n",
    "We need to tokenize text, meaning break down into smaller pieces. The most common tokenization technique is to break down text into words. We can do this using scikit-learn's CountVectorizer, where every row will represent a different document and every column will represent a different word.\n",
    "\n",
    "In addition, with CountVectorizer, we can remove stop words. Stop words are common words that add no additional meaning to text such as 'a', 'the', etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4720x7786 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 37280 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We are going to create a document-term matrix using CountVectorizer, and exclude common English stop words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "data_cv = cv.fit_transform(clean_data2)\n",
    "data_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to a numerical array\n",
    "data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_dtm.index = clean_data2.index\n",
    "# Let's pickle it for later use\n",
    "data_dtm.to_pickle(\"data/title30_dtm.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also pickle the cleaned data (before we put it in document-term matrix format) and the CountVectorizer object\n",
    "clean_data2.to_pickle('data/title30_clean_data2.pkl')\n",
    "pickle.dump(cv, open(\"data/title30_cv.pkl\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
