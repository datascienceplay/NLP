{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling\n",
    "\n",
    "The ultimate goal of topic modeling is to find various topics that are present in COVID-19 publications.\n",
    "\n",
    "Latent Dirichlet Allocation (LDA) is one of many topic modeling techniques. It was specifically designed for text data.\n",
    "\n",
    "To use a topic modeling technique, we need (1) a document-term matrix and (2) the number of topics you would like the algorithm to pick up.\n",
    "\n",
    "Once the topic modeling technique is applied, we need to interpret the results and see if the mix of words in each topic make sense. If they don't make sense, we will try changing up the number of topics, the terms in the document-term matrix, model parameters, or even try a different model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "dr = pd.read_pickle('data/covid19_reference_count.pickle')\n",
    "abstract = pd.read_pickle('data/covid19_abstract.pickle')\n",
    "abstract30 = abstract[dr>=30]\n",
    "abstract30_text = abstract30.astype('str')\n",
    "\n",
    "#clean data first\n",
    "#Apply a first round of text cleaning techniques\n",
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text_round1(text):\n",
    "    '''Make text lowercase, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\<.*?\\>', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "round1 = lambda x: clean_text_round1(x)\n",
    "clean_data1 = abstract30_text.apply(round1)\n",
    "\n",
    "# Apply a second round of cleaning\n",
    "def clean_text_round2(text):\n",
    "    '''Get rid of some additional punctuation and non-sensical text that was missed the first time around.'''\n",
    "    text = re.sub('[‘’“”…]', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    return text\n",
    "\n",
    "round2 = lambda x: clean_text_round2(x)\n",
    "\n",
    "# Let's take a look at the updated text\n",
    "clean_data2 = clean_data1.apply(round2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create our document-term matrix from abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a document-term matrix using CountVectorizer, and exclude common English stop words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords;\n",
    "\n",
    "#new_stopwords =['pandemic','study','nan','patient','disease','coronavirus',\n",
    "#                'health','review','impact','research','may','results',\n",
    "#               'public']\n",
    "new_stopwords = ['']\n",
    "new_stopwords += stopwords.words('english')\n",
    "cv = CountVectorizer(stop_words=new_stopwords)\n",
    "data_cv = cv.fit_transform(clean_data2)\n",
    "\n",
    "# convert to a numerical array\n",
    "data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_dtm.index = clean_data2.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules for LDA with gensim\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse\n",
    "# One of the required inputs is a term-document matrix\n",
    "tdm = data_dtm.transpose()\n",
    "# We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\n",
    "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term), we need to specify two other parameters - the number of topics and the number of passes. Let's start the number of topics at 2, see if the results make sense, and increase the number from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.007*\"pandemic\" + 0.007*\"health\" + 0.006*\"coronavirus\" + 0.005*\"virus\" + 0.005*\"protein\" + 0.004*\"study\" + 0.004*\"viral\" + 0.004*\"data\" + 0.004*\"disease\" + 0.003*\"human\"'),\n",
       " (1,\n",
       "  '0.036*\"nan\" + 0.013*\"patients\" + 0.008*\"infection\" + 0.008*\"cells\" + 0.007*\"disease\" + 0.006*\"respiratory\" + 0.005*\"severe\" + 0.005*\"viral\" + 0.005*\"expression\" + 0.004*\"coronavirus\"')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term),\n",
    "# we need to specify two other parameters as well - the number of topics and the number of passes\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.011*\"health\" + 0.008*\"pandemic\" + 0.005*\"study\" + 0.005*\"research\" + 0.005*\"data\" + 0.005*\"social\" + 0.004*\"public\" + 0.004*\"results\" + 0.003*\"care\" + 0.003*\"disease\"'),\n",
       " (1,\n",
       "  '0.009*\"viral\" + 0.008*\"protein\" + 0.008*\"virus\" + 0.007*\"coronavirus\" + 0.007*\"human\" + 0.006*\"cells\" + 0.006*\"infection\" + 0.005*\"pandemic\" + 0.005*\"spike\" + 0.005*\"disease\"'),\n",
       " (2,\n",
       "  '0.075*\"nan\" + 0.020*\"patients\" + 0.008*\"disease\" + 0.007*\"de\" + 0.006*\"clinical\" + 0.006*\"severe\" + 0.006*\"infection\" + 0.006*\"risk\" + 0.005*\"respiratory\" + 0.005*\"coronavirus\"')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 3\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.011*\"health\" + 0.008*\"pandemic\" + 0.005*\"study\" + 0.005*\"research\" + 0.005*\"data\" + 0.004*\"social\" + 0.004*\"care\" + 0.004*\"patients\" + 0.004*\"results\" + 0.004*\"public\"'),\n",
       " (1,\n",
       "  '0.012*\"coronavirus\" + 0.008*\"respiratory\" + 0.007*\"disease\" + 0.007*\"virus\" + 0.007*\"de\" + 0.006*\"patients\" + 0.005*\"severe\" + 0.005*\"pandemic\" + 0.005*\"syndrome\" + 0.005*\"la\"'),\n",
       " (2,\n",
       "  '0.011*\"protein\" + 0.009*\"virus\" + 0.009*\"viral\" + 0.007*\"coronavirus\" + 0.007*\"human\" + 0.006*\"spike\" + 0.005*\"pandemic\" + 0.005*\"host\" + 0.005*\"binding\" + 0.005*\"infection\"'),\n",
       " (3,\n",
       "  '0.071*\"nan\" + 0.013*\"cells\" + 0.009*\"patients\" + 0.009*\"infection\" + 0.007*\"expression\" + 0.007*\"disease\" + 0.006*\"lung\" + 0.006*\"respiratory\" + 0.005*\"de\" + 0.005*\"severe\"')]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 4\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments: These topics aren't looking too great. I tried to add some stop words. \n",
    "Let's try modifying our terms list as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try topic modelling using Nouns only from the abstracts\n",
    "\n",
    "One popular trick is to look only at terms that are from one part of speech (only nouns, only adjectives, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    return ' '.join(all_nouns)\n",
    "\n",
    "# Apply the nouns function to the abstracts to filter only on nouns\n",
    "data_nouns = pd.Series(clean_data2.apply(nouns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new document-term matrix using only nouns\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#new_stopwords =['pandemic','study','nan','patient','disease','coronavirus',\n",
    "#                'health','review','impact','research','may','results',\n",
    "#               'public']\n",
    "new_stopwords=['da','la','em','se','que','por','nan']\n",
    "new_stopwords += stopwords.words('english')\n",
    "# Recreate a document-term matrix with only nouns\n",
    "cvn = CountVectorizer(stop_words=new_stopwords)\n",
    "data_cvn = cvn.fit_transform(data_nouns)\n",
    "data_dtmn = pd.DataFrame(data_cvn.toarray(), columns=cvn.get_feature_names())\n",
    "data_dtmn.index = data_nouns.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordn = dict((v, k) for k, v in cvn.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.013*\"protein\" + 0.012*\"virus\" + 0.012*\"coronavirus\" + 0.011*\"infection\" + 0.011*\"cells\" + 0.008*\"disease\" + 0.008*\"cell\" + 0.007*\"respiratory\" + 0.007*\"host\" + 0.007*\"spike\"'),\n",
       " (1,\n",
       "  '0.017*\"health\" + 0.011*\"patients\" + 0.008*\"data\" + 0.008*\"disease\" + 0.008*\"study\" + 0.008*\"research\" + 0.006*\"coronavirus\" + 0.006*\"results\" + 0.006*\"pandemic\" + 0.006*\"care\"')]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=2, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.014*\"protein\" + 0.014*\"virus\" + 0.013*\"infection\" + 0.012*\"coronavirus\" + 0.012*\"cells\" + 0.010*\"disease\" + 0.009*\"cell\" + 0.008*\"respiratory\" + 0.008*\"host\" + 0.007*\"spike\"'),\n",
       " (1,\n",
       "  '0.009*\"drugs\" + 0.007*\"drug\" + 0.007*\"research\" + 0.006*\"coronavirus\" + 0.006*\"inhibitors\" + 0.005*\"compounds\" + 0.005*\"protease\" + 0.004*\"analysis\" + 0.004*\"results\" + 0.004*\"studies\"'),\n",
       " (2,\n",
       "  '0.020*\"health\" + 0.012*\"patients\" + 0.008*\"study\" + 0.008*\"disease\" + 0.008*\"data\" + 0.007*\"care\" + 0.007*\"research\" + 0.006*\"pandemic\" + 0.006*\"coronavirus\" + 0.006*\"results\"')]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try topics = 3\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=3, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.015*\"protein\" + 0.014*\"virus\" + 0.013*\"cells\" + 0.013*\"coronavirus\" + 0.012*\"infection\" + 0.009*\"cell\" + 0.009*\"disease\" + 0.008*\"host\" + 0.008*\"spike\" + 0.007*\"respiratory\"'),\n",
       " (1,\n",
       "  '0.029*\"health\" + 0.013*\"research\" + 0.010*\"study\" + 0.008*\"data\" + 0.006*\"care\" + 0.006*\"information\" + 0.006*\"pandemic\" + 0.005*\"results\" + 0.005*\"media\" + 0.005*\"management\"'),\n",
       " (2,\n",
       "  '0.024*\"patients\" + 0.014*\"disease\" + 0.010*\"coronavirus\" + 0.010*\"cases\" + 0.008*\"risk\" + 0.008*\"data\" + 0.008*\"infection\" + 0.008*\"health\" + 0.007*\"treatment\" + 0.007*\"results\"'),\n",
       " (3,\n",
       "  '0.005*\"research\" + 0.005*\"study\" + 0.005*\"generation\" + 0.004*\"authors\" + 0.004*\"results\" + 0.004*\"consumption\" + 0.004*\"data\" + 0.004*\"pandemic\" + 0.004*\"information\" + 0.004*\"estate\"')]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try topics = 4\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=4, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try topic modelling using Nouns + Adjectives from the abstracts\n",
    "\n",
    "One popular trick is to look only at terms that are from one part of speech (only nouns, only adjectives, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "def nouns_adj(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
    "    return ' '.join(nouns_adj)\n",
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns_adj = pd.DataFrame(data_clean.transcript.apply(nouns_adj))\n",
    "# Create a new document-term matrix using only nouns and adjectives, also remove common words with max_df\n",
    "cvna = CountVectorizer(stop_words=stop_words, max_df=.8)\n",
    "data_cvna = cvna.fit_transform(data_nouns_adj.transcript)\n",
    "data_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names())\n",
    "data_dtmna.index = data_nouns_adj.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with 2 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=2, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with 3 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=3, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with 4 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "further work to be done:\n",
    "1. more data cleaning, e.g., cells and cell should be one word.\n",
    "2. try to add more stop words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
